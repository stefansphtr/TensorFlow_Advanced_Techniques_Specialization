{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWqcoPhU3RJN"
   },
   "source": [
    "# Breast Cancer Prediction\n",
    "\n",
    "In this exercise, you will train a neural network on the [Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)) to predict if the tumor is malignant or benign.\n",
    "\n",
    "If you get stuck, we recommend that you review the ungraded labs for this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "st5AIBFZ5mEQ"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JkMXve8XuN5X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUc3HpEQ5s6U"
   },
   "source": [
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-TQFUXu5wS_"
   },
   "source": [
    "We first load the dataset and create a data frame using pandas. We explicitly specify the column names because the CSV file does not have column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVh-W73J5TjS"
   },
   "outputs": [],
   "source": [
    "data_file = './data/data.csv'\n",
    "col_names = [\"id\", \"clump_thickness\", \"un_cell_size\", \"un_cell_shape\", \"marginal_adheshion\", \"single_eph_cell_size\", \"bare_nuclei\", \"bland_chromatin\", \"normal_nucleoli\", \"mitoses\", \"class\"]\n",
    "df = pd.read_csv(data_file, names=col_names, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEv8vS_P6HaV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>un_cell_size</th>\n",
       "      <th>un_cell_shape</th>\n",
       "      <th>marginal_adheshion</th>\n",
       "      <th>single_eph_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chromatin</th>\n",
       "      <th>normal_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  clump_thickness  un_cell_size  un_cell_shape  marginal_adheshion  \\\n",
       "0  1000025                5             1              1                   1   \n",
       "1  1002945                5             4              4                   5   \n",
       "2  1015425                3             1              1                   1   \n",
       "3  1016277                6             8              8                   1   \n",
       "4  1017023                4             1              1                   3   \n",
       "\n",
       "   single_eph_cell_size bare_nuclei  bland_chromatin  normal_nucleoli  \\\n",
       "0                     2           1                3                1   \n",
       "1                     7          10                3                2   \n",
       "2                     2           2                3                1   \n",
       "3                     3           4                3                7   \n",
       "4                     2           1                3                1   \n",
       "\n",
       "   mitoses  class  \n",
       "0        1      2  \n",
       "1        1      2  \n",
       "2        1      2  \n",
       "3        1      2  \n",
       "4        1      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvvbnFL36L85"
   },
   "source": [
    "We have to do some preprocessing on the data. We first pop the id column since it is of no use for our problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDeXwHdA5uUN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1000025\n",
       "1      1002945\n",
       "2      1015425\n",
       "3      1016277\n",
       "4      1017023\n",
       "        ...   \n",
       "694     776715\n",
       "695     841769\n",
       "696     888820\n",
       "697     897471\n",
       "698     897471\n",
       "Name: id, Length: 699, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pop(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubw5LueA6ZEY"
   },
   "source": [
    "Upon inspection of data, you can see that some values of the **bare_nuclei** column are unknown. We drop the rows with these unknown values. We also convert the **bare_nuclei** column to numeric. This is required for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCcOrl1ITVhr"
   },
   "outputs": [],
   "source": [
    "df = df[df[\"bare_nuclei\"] != '?' ]\n",
    "df.bare_nuclei = pd.to_numeric(df.bare_nuclei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQMhcTQG7LzY"
   },
   "source": [
    "We check the class distribution of the data. You can see that there are two classes, 2.0 and 4.0\n",
    "According to the dataset:\n",
    "* **2.0 = benign**\n",
    "* **4.0 = malignant**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SaAdQrBv8daS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['class'].hist(bins=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENjMKvxQ6sWy"
   },
   "source": [
    "We are going to model this problem as a binary classification problem which detects whether the tumor is malignant or not. Hence, we change the dataset so that:\n",
    "* **benign(2.0) = 0**\n",
    "* **malignant(4.0) = 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MVzeUwf_A3E",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df['class'] = np.where(df['class'] == 2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGbKO1bR8S9h"
   },
   "source": [
    "We then split the dataset into training and testing sets. Since the number of samples is small, we will perform validation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNUy7JcuAXjC"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_ZKokUP8kP3"
   },
   "source": [
    "We get the statistics for training. We can look at statistics to get an idea about the distribution of plots. If you need more visualization, you can create additional data plots. We will also be using the mean and standard deviation from statistics for normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k86tBT_QAm2P"
   },
   "outputs": [],
   "source": [
    "train_stats = train.describe()\n",
    "train_stats.pop('class')\n",
    "train_stats = train_stats.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l8AJ0Crc8u9t"
   },
   "source": [
    "We pop the class column from the training and test sets to create train and test outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7EGUV-tA5LZ"
   },
   "outputs": [],
   "source": [
    "train_Y = train.pop(\"class\")\n",
    "test_Y = test.pop(\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9wVRO5E9AgA"
   },
   "source": [
    "Here we normalize the data by using the formula: **X = (X - mean(X)) / StandardDeviation(X)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NDo__q_AA3j0"
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdARlWaDA_8G"
   },
   "outputs": [],
   "source": [
    "norm_train_X = norm(train)\n",
    "norm_test_X = norm(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6LIVZbj9Usv"
   },
   "source": [
    "We now create Tensorflow datasets for training and test sets to easily be able to build and manage an input pipeline for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1S0RtsP1Xsj8"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((norm_train_X.values, train_Y.values))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((norm_test_X.values, test_Y.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Nb44PpV9hR4"
   },
   "source": [
    "We shuffle and prepare a batched dataset to be used for training in our custom training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9qdsNPen5-F"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train)).batch(batch_size)\n",
    "\n",
    "test_dataset =  test_dataset.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "a = enumerate(train_dataset)\n",
    "\n",
    "print(len(list(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcbOJ6C79qT5"
   },
   "source": [
    "## Define the Model\n",
    "\n",
    "Now we will define the model. Here, we use the Keras Functional API to create a simple network of two `Dense` layers. We have modelled the problem as a binary classification problem and hence we add a single layer with sigmoid activation as the final layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HU3qcM9WBcMh"
   },
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    inputs = tf.keras.layers.Input(shape=(len(train.columns)))\n",
    "\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = base_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NBhKIcKQ-Bwe"
   },
   "source": [
    "## Define Optimizer and Loss\n",
    "\n",
    "We use RMSprop optimizer and binary crossentropy as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5B3vh6fs84i"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSNDewgovSZ8"
   },
   "source": [
    "## Evaluate Untrained Model\n",
    "We calculate the loss on the model before training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUScS3GbtPXt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before training 0.7074\n"
     ]
    }
   ],
   "source": [
    "outputs = model(norm_test_X.values)\n",
    "loss_value = loss_object(y_true=test_Y.values.reshape(-1, 1), y_pred=outputs)\n",
    "print(\"Loss before training %.4f\" % loss_value.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPPb5ewkzMBY"
   },
   "source": [
    "We also plot the confusion matrix to visualize the true outputs against the outputs predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueenYwWZvQM_"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title='', labels=[0,1]):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title(title)\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "          plt.text(j, i, format(cm[i, j], fmt),\n",
    "                  horizontalalignment=\"center\",\n",
    "                  color=\"black\" if cm[i, j] > thresh else \"white\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FApnBUNWv-ZR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1912\\3073957586.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + labels)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1912\\3073957586.py:9: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + labels)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(test_Y.values, tf.round(outputs), title='Confusion Matrix for Untrained Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-HTkbQb-gYp"
   },
   "source": [
    "## Define Metrics (Please complete this section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYUyRka1-j87"
   },
   "source": [
    "### Define Custom F1Score Metric\n",
    "In this example, we will define a custom F1Score metric using the formula. \n",
    "\n",
    "**F1 Score = 2 * ((precision * recall) / (precision + recall))**\n",
    "\n",
    "**precision = true_positives / (true_positives + false_positives)**\n",
    "\n",
    "**recall = true_positives / (true_positives + false_negatives)**\n",
    "\n",
    "We use `confusion_matrix` defined in `tf.math` to calculate precision and recall.\n",
    "\n",
    "Here you can see that we have subclassed `tf.keras.Metric` and implemented the three required methods `update_state`, `result` and `reset_states`.\n",
    "\n",
    "### Please complete the result() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdUe6cqvbzXy"
   },
   "outputs": [],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        '''initializes attributes of the class'''\n",
    "        \n",
    "        # call the parent class init\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "\n",
    "        # Initialize Required variables\n",
    "        # true positives\n",
    "        self.tp = tf.Variable(0, dtype = 'int32')\n",
    "        # false positives\n",
    "        self.fp = tf.Variable(0, dtype = 'int32')\n",
    "        # true negatives\n",
    "        self.tn = tf.Variable(0, dtype = 'int32')\n",
    "        # false negatives\n",
    "        self.fn = tf.Variable(0, dtype = 'int32')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        '''\n",
    "        Accumulates statistics for the metric\n",
    "        \n",
    "        Args:\n",
    "            y_true: target values from the test data\n",
    "            y_pred: predicted values by the model\n",
    "        '''\n",
    "\n",
    "        # Calulcate confusion matrix.\n",
    "        conf_matrix = tf.math.confusion_matrix(y_true, y_pred, num_classes=2)\n",
    "        \n",
    "        # Update values of true positives, true negatives, false positives and false negatives from confusion matrix.\n",
    "        self.tn.assign_add(conf_matrix[0][0])\n",
    "        self.tp.assign_add(conf_matrix[1][1])\n",
    "        self.fp.assign_add(conf_matrix[0][1])\n",
    "        self.fn.assign_add(conf_matrix[1][0])\n",
    "\n",
    "    def result(self):\n",
    "        '''Computes and returns the metric value tensor.'''\n",
    "\n",
    "        # Calculate precision\n",
    "        if (self.tp + self.fp == 0):\n",
    "            precision = 1.0\n",
    "        else:\n",
    "            precision = self.tp / (self.tp + self.fp)\n",
    "      \n",
    "        # Calculate recall\n",
    "        if (self.tp + self.fn == 0):\n",
    "            recall = 1.0\n",
    "        else:\n",
    "            recall = self.tp / (self.tp + self.fn)\n",
    "\n",
    "        # Return F1 Score\n",
    "        ### START CODE HERE ###\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return f1_score\n",
    "\n",
    "    def reset_states(self):\n",
    "        '''Resets all of the metric state variables.'''\n",
    "        \n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.tp.assign(0)\n",
    "        self.tn.assign(0) \n",
    "        self.fp.assign(0)\n",
    "        self.fn.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.2222222222222222>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Code:\n",
    "\n",
    "test_F1Score = F1Score()\n",
    "\n",
    "test_F1Score.tp = tf.Variable(2, dtype = 'int32')\n",
    "test_F1Score.fp = tf.Variable(5, dtype = 'int32')\n",
    "test_F1Score.tn = tf.Variable(7, dtype = 'int32')\n",
    "test_F1Score.fn = tf.Variable(9, dtype = 'int32')\n",
    "test_F1Score.result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```txt\n",
    "<tf.Tensor: shape=(), dtype=float64, numpy=0.2222222222222222>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiTa2CePAOTa"
   },
   "source": [
    "We initialize the seprate metrics required for training and validation. In addition to our custom F1Score metric, we are also using `BinaryAccuracy` defined in `tf.keras.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Pa_x-5-CH_V"
   },
   "outputs": [],
   "source": [
    "train_f1score_metric = F1Score()\n",
    "val_f1score_metric = F1Score()\n",
    "\n",
    "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1huOxRpEAxvf"
   },
   "source": [
    "## Apply Gradients (Please complete this section)\n",
    "\n",
    "The core of training is using the model to calculate the logits on specific set of inputs and compute the loss(in this case **binary crossentropy**) by comparing the predicted outputs to the true outputs. We then update the trainable weights using the optimizer algorithm chosen. The optimizer algorithm requires our computed loss and partial derivatives of loss with respect to each of the trainable weights to make updates to the same.\n",
    "\n",
    "We use gradient tape to calculate the gradients and then update the model trainable weights using the optimizer.\n",
    "\n",
    "### Please complete the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMPe25Dstn0v"
   },
   "outputs": [],
   "source": [
    "def apply_gradient(optimizer, loss_object, model, x, y):\n",
    "    '''\n",
    "    applies the gradients to the trainable model weights\n",
    "    \n",
    "    Args:\n",
    "        optimizer: optimizer to update model weights\n",
    "        loss_object: type of loss to measure during training\n",
    "        model: the model we are training\n",
    "        x: input data to the model\n",
    "        y: target values for each input\n",
    "    '''\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "    ### START CODE HERE ###\n",
    "        logits = model(x)\n",
    "        loss_value = loss_object(y_true=y, y_pred=logits)\n",
    "  \n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    ### END CODE HERE ###\n",
    "  \n",
    "    return logits, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53152144]\n",
      " [0.5279946 ]\n",
      " [0.50008744]\n",
      " [0.57553554]\n",
      " [0.45085052]\n",
      " [0.53419566]\n",
      " [0.51514876]\n",
      " [0.51763475]]\n",
      "0.7494387\n"
     ]
    }
   ],
   "source": [
    "# Test Code:\n",
    "\n",
    "test_model = tf.keras.models.load_model('./test_model')\n",
    "test_logits, test_loss = apply_gradient(optimizer, loss_object, test_model, norm_test_X.values, test_Y.values.reshape(-1, 1))\n",
    "\n",
    "print(test_logits.numpy()[:8])\n",
    "print(test_loss.numpy())\n",
    "\n",
    "del test_model\n",
    "del test_logits\n",
    "del test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "The output will be close to these values:\n",
    "```txt\n",
    "[[0.5516499 ]\n",
    " [0.52124363]\n",
    " [0.5412698 ]\n",
    " [0.54203206]\n",
    " [0.50022954]\n",
    " [0.5459626 ]\n",
    " [0.47841492]\n",
    " [0.54381996]]\n",
    "0.7030578\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYM6GZPjB40r"
   },
   "source": [
    "## Training Loop (Please complete this section)\n",
    "\n",
    "This function performs training during one epoch. We run through all batches of training data in each epoch to make updates to trainable weights using our previous function.\n",
    "You can see that we also call `update_state` on our metrics to accumulate the value of our metrics. \n",
    "\n",
    "We are displaying a progress bar to indicate completion of training in each epoch. Here we use `tqdm` for displaying the progress bar. \n",
    "\n",
    "### Please complete the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fHoh_hgz2PC"
   },
   "outputs": [],
   "source": [
    "def train_data_for_one_epoch(train_dataset, optimizer, loss_object, model, \n",
    "                             train_acc_metric, train_f1score_metric, verbose=True):\n",
    "    '''\n",
    "    Computes the loss then updates the weights and metrics for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset: the training dataset\n",
    "        optimizer: optimizer to update model weights\n",
    "        loss_object: type of loss to measure during training\n",
    "        model: the model we are training\n",
    "        train_acc_metric: calculates how often predictions match labels\n",
    "        train_f1score_metric: custom metric we defined earlier\n",
    "    '''\n",
    "    # Build the optimizer with the full list of trainable variables\n",
    "    losses = []\n",
    "\n",
    "    #Iterate through all batches of training data\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        #Calculate loss and update trainable variables using optimizer\n",
    "        ### START CODE HERE ###\n",
    "        logits, loss_value = apply_gradient(optimizer, loss_object, model, x_batch_train, y_batch_train)\n",
    "        losses.append(loss_value)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        #Round off logits to nearest integer and cast to integer for calulating metrics\n",
    "        logits = tf.round(logits)\n",
    "        logits = tf.cast(logits, 'int64')\n",
    "\n",
    "        #Update the training metrics\n",
    "        ### START CODE HERE ###\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "        train_f1score_metric.update_state(y_batch_train, logits)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        #Update progress\n",
    "        if verbose:\n",
    "            print(\"Training loss for step %s: %.4f\" % (int(step), float(loss_value)))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'The optimizer cannot recognize variable dense_24/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.RMSprop.'\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE\n",
    "\n",
    "test_model = tf.keras.models.load_model('./test_model')\n",
    "\n",
    "try:\n",
    "    test_losses = train_data_for_one_epoch(train_dataset, optimizer, loss_object, test_model, \n",
    "                             train_acc_metric, train_f1score_metric, verbose=False)\n",
    "    for test_loss in test_losses:\n",
    "        print(test_loss.numpy())\n",
    "        \n",
    "    del test_model\n",
    "    del test_losses\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "The losses should generally be decreasing and will start from around 0.75. For example:\n",
    "\n",
    "```\n",
    "0.7600615\n",
    "0.6092045\n",
    "0.5525634\n",
    "0.4358902\n",
    "0.4765755\n",
    "0.43327087\n",
    "0.40585428\n",
    "0.32855004\n",
    "0.35755336\n",
    "0.3651728\n",
    "0.33971977\n",
    "0.27372319\n",
    "0.25026917\n",
    "0.29229593\n",
    "0.242178\n",
    "0.20602849\n",
    "0.15887335\n",
    "0.090397514\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9RJq8BLCsSF"
   },
   "source": [
    "At the end of each epoch, we have to validate the model on the test dataset. The following function calculates the loss on test dataset and updates the states of the validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gLJyAJE0YRc"
   },
   "outputs": [],
   "source": [
    "def perform_validation():\n",
    "    losses = []\n",
    "\n",
    "    #Iterate through all batches of validation data.\n",
    "    for x_val, y_val in test_dataset:\n",
    "\n",
    "        #Calculate validation loss for current batch.\n",
    "        val_logits = model(x_val) \n",
    "        val_loss = loss_object(y_true=y_val, y_pred=val_logits)\n",
    "        losses.append(val_loss)\n",
    "\n",
    "        #Round off and cast outputs to either  or 1\n",
    "        val_logits = tf.cast(tf.round(model(x_val)), 'int64')\n",
    "\n",
    "        #Update validation metrics\n",
    "        val_acc_metric.update_state(y_val, val_logits)\n",
    "        val_f1score_metric.update_state(y_val, val_logits)\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLymSCkUC-CL"
   },
   "source": [
    "Next we define the training loop that runs through the training samples repeatedly over a fixed number of epochs. Here we combine the functions we built earlier to establish the following flow:\n",
    "1. Perform training over all batches of training data.\n",
    "2. Get values of metrics.\n",
    "3. Perform validation to calculate loss and update validation metrics on test data.\n",
    "4. Reset the metrics at the end of epoch.\n",
    "5. Display statistics at the end of each epoch.\n",
    "\n",
    "**Note** : We also calculate the training and validation losses for the whole epoch at the end of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOO1x3VyuPUV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'The optimizer cannot recognize variable dense/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.RMSprop.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart of epoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch,))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Perform Training over all batches of train data\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m losses_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data_for_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_acc_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_f1score_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Get results from training metrics\u001b[39;00m\n\u001b[0;32m     11\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m train_acc_metric\u001b[38;5;241m.\u001b[39mresult()\n",
      "Cell \u001b[1;32mIn[40], line 24\u001b[0m, in \u001b[0;36mtrain_data_for_one_epoch\u001b[1;34m(train_dataset, optimizer, loss_object, model, train_acc_metric, train_f1score_metric, verbose)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Iterate through all batches of training data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (x_batch_train, y_batch_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#Calculate loss and update trainable variables using optimizer\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m### START CODE HERE ###\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     logits, loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mapply_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss_value)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m#Round off logits to nearest integer and cast to integer for calulating metrics\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m, in \u001b[0;36mapply_gradient\u001b[1;34m(optimizer, loss_object, model, x, y)\u001b[0m\n\u001b[0;32m     16\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss_object(y_true\u001b[38;5;241m=\u001b[39my, y_pred\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[0;32m     18\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_value, model\u001b[38;5;241m.\u001b[39mtrainable_weights)\n\u001b[1;32m---> 19\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss_value\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1223\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1222\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    651\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 652\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1253\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_with_dtensor:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;66;03m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m-> 1253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[1;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[0;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1345\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1345\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[0;32m   1350\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3013\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3010\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   3011\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3012\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 3013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3015\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   3016\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4083\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[0;32m   4081\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[0;32m   4082\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[1;32m-> 4083\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4089\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4085\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[0;32m   4086\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[0;32m   4087\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[0;32m   4088\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[1;32m-> 4089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[0;32m   4091\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1342\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step_xla(grad, var, \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:233\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(variable) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict:\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimizer cannot recognize variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate different parts of the model separately. Please call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`optimizer.build(variables)` with the full list of trainable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables before the training loop or use legacy optimizer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(gradient, variable)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'The optimizer cannot recognize variable dense/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.RMSprop.'"
     ]
    }
   ],
   "source": [
    "# Iterate over epochs.\n",
    "epochs = 5\n",
    "epochs_val_losses, epochs_train_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    #Perform Training over all batches of train data\n",
    "    losses_train = train_data_for_one_epoch(train_dataset, optimizer, loss_object, model, train_acc_metric, train_f1score_metric)\n",
    "\n",
    "    # Get results from training metrics\n",
    "    train_acc = train_acc_metric.result()\n",
    "    train_f1score = train_f1score_metric.result()\n",
    "\n",
    "    #Perform validation on all batches of test data\n",
    "    losses_val = perform_validation()\n",
    "\n",
    "    # Get results from validation metrics\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_f1score = val_f1score_metric.result()\n",
    "\n",
    "    #Calculate training and validation losses for current epoch\n",
    "    losses_train_mean = np.mean(losses_train)\n",
    "    losses_val_mean = np.mean(losses_val)\n",
    "    epochs_val_losses.append(losses_val_mean)\n",
    "    epochs_train_losses.append(losses_train_mean)\n",
    "\n",
    "    print('\\n Epcoh %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f, Train F1 Score: %.4f, Validation F1 Score: %.4f' % (epoch, float(losses_train_mean), float(losses_val_mean), float(train_acc), float(val_acc), train_f1score, val_f1score))\n",
    "\n",
    "    #Reset states of all metrics\n",
    "    train_acc_metric.reset_states()\n",
    "    val_acc_metric.reset_states()\n",
    "    val_f1score_metric.reset_states()\n",
    "    train_f1score_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoLxueMdzm14"
   },
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6EGW3HVUzqBX"
   },
   "source": [
    "### Plots for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8Wsr6wG0T4h"
   },
   "source": [
    "We plot the progress of loss as training proceeds over number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsmF_2n307SP"
   },
   "outputs": [],
   "source": [
    "def plot_metrics(train_metric, val_metric, metric_name, title, ylim=5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "    plt.plot(train_metric,color='blue',label=metric_name)\n",
    "    plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
    "\n",
    "plot_metrics(epochs_train_losses, epochs_val_losses, \"Loss\", \"Loss\", ylim=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27fXX7Yqyu5S"
   },
   "source": [
    "We plot the confusion matrix to visualize the true values against the values predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9n2XJ9MwpDS"
   },
   "outputs": [],
   "source": [
    "test_outputs = model(norm_test_X.values)\n",
    "plot_confusion_matrix(test_Y.values, tf.round(test_outputs), title='Confusion Matrix for Untrained Model')"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "TF3C2W2-1",
    "TF3C2W2-2",
    "TF3C2W2-3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
